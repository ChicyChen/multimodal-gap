{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most commonly used\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "from tqdm import tqdm, trange\n",
    "from colors import blue, red, green, cyan\n",
    "\n",
    "# Numerical computation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "sys.path.append('ANONYMOUS_ROOTDIR/develop/open-world/')\n",
    "from utils import svd, reduce_and_visualize, load_clip, encode_clip, encode_clip_classification, train_clip_toy, ce_loss, uniform_loss, dual_ce_loss, simple_ce_loss\n",
    "from datasets import ImageCaptionDataset, ClassificationDataset\n",
    "\n",
    "\n",
    "def evaluate_retrieval(image_features, text_features):\n",
    "    metrics = {}\n",
    "    sim = image_features @ text_features.T\n",
    "    for K in [1, 5, 10]:\n",
    "        pred = sim.argsort(dim=-1)\n",
    "        text_r = np.mean([i in pred[i, -K:] for i in range(len(pred))])\n",
    "\n",
    "        pred = sim.argsort(dim=0)\n",
    "        image_r = np.mean([i in pred[-K:, i] for i in range(len(pred))])\n",
    "\n",
    "        metrics[f'Text R@{K}'] = text_r\n",
    "        metrics[f'Image R@{K}'] = image_r\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_classification(image_features, text_features, labels):\n",
    "    metrics = {}\n",
    "    sim = image_features @ text_features.T\n",
    "    for K in [1, 5, 10]:\n",
    "        pred = sim.argsort(dim=-1)\n",
    "        text_r = np.mean([labels[i] in pred[i, -K:] for i in range(len(pred))])\n",
    "        metrics[f'Hit@{K}'] = text_r\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_binary_classification(image_features, text_features, labels):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    metrics = {}\n",
    "    sim = image_features @ text_features.T * 100\n",
    "    probs = F.softmax(sim, dim=-1)[:, 1]\n",
    "    roc_auc = roc_auc_score(labels, probs)\n",
    "    metrics[f'ROC-AUC'] = roc_auc\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def move_features(image_features, text_features, evaluate_func, direction_vec=None):\n",
    "    all_metrics = {}\n",
    "    if direction_vec is None:\n",
    "        modality_gap = image_features.mean(axis=0) - text_features.mean(axis=0)\n",
    "        modality_gap = modality_gap / modality_gap.norm()\n",
    "        direction_vec = modality_gap\n",
    "    \n",
    "    for delta in np.arange(-5, 5, 0.25):\n",
    "        modified_text_features = text_features + 0.5 * delta * direction_vec\n",
    "        modified_text_features /= modified_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        modified_image_features = image_features - 0.5 * delta * direction_vec\n",
    "        modified_image_features /= modified_image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # reduce_and_visualize(modified_image_features.numpy(), modified_text_features.numpy(), methods=['svd', 'pca'], n_dim=2)\n",
    "\n",
    "        preds = (modified_image_features @ modified_text_features.T).argmax(dim=-1)\n",
    "\n",
    "        gap_distance = (modified_text_features.mean(axis=0) - modified_image_features.mean(axis=0)).norm().item()\n",
    "\n",
    "        metrics = evaluate_func(modified_image_features, modified_text_features)\n",
    "        all_metrics[delta] = (metrics, gap_distance, preds)\n",
    "\n",
    "        print(delta, metrics, gap_distance)\n",
    "    return all_metrics\n",
    "\n",
    "\n",
    "def move_features_along_hypersphere(image_features, text_features, evaluate_func):\n",
    "    return \"Impossible\"\n",
    "\n",
    "\n",
    "def plot_metrics(all_metrics, metric_name='Hit@1'):\n",
    "    xs, ys = [], []\n",
    "    for delta in sorted(all_metrics.keys()):\n",
    "        metrics, gap_distance, preds = all_metrics[delta]\n",
    "        xs.append(gap_distance)\n",
    "        ys.append(metrics[metric_name])\n",
    "    print(f'Optimal {metric_name}: {max(ys)}')\n",
    "\n",
    "    minidx = xs.index(min(xs))\n",
    "    for i in range(minidx + 1, len(xs)): xs[i] = -xs[i]\n",
    "    plt.plot(xs, ys, 'o-')\n",
    "    plt.xlabel('Gap Distance')\n",
    "    plt.ylabel(metric_name)\n",
    "\n",
    "    initial_gap = all_metrics[0][1]\n",
    "    plt.axvline(initial_gap, color='k', linestyle='--')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move features along direction computed on downstream tasks\n",
    "\n",
    "model = load_clip()\n",
    "dataset = ClassificationDataset(name='EuroSAT')\n",
    "image_features, text_features = encode_clip_classification(model, dataset, prompt='a centered satellite photo of {}.')\n",
    "labels = [item[1] for item in dataset]\n",
    "metrics = evaluate_classification(image_features, text_features, labels)\n",
    "print(metrics)\n",
    "\n",
    "reduce_and_visualize(image_features.numpy(), text_features.numpy(), methods=['svd', 'pca', 'tsne', 'umap'], n_dim=2)\n",
    "\n",
    "all_metrics = move_features(image_features, text_features, partial(evaluate_classification, labels=labels))\n",
    "plot_metrics(all_metrics, metric_name='Hit@1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move features along direction computed on MSCOCO\n",
    "\n",
    "model = load_clip()\n",
    "\n",
    "dataset = ImageCaptionDataset()\n",
    "image_features, text_features = encode_clip(model, dataset)\n",
    "direction_vec = image_features.mean(axis=0) - text_features.mean(axis=0)\n",
    "direction_vec = direction_vec / direction_vec.norm()\n",
    "\n",
    "dataset = ClassificationDataset(name='SVHN')\n",
    "image_features, text_features = encode_clip_classification(model, dataset, prompt='a street sign of the number: \"{}\".')\n",
    "labels = [item[1] for item in dataset]\n",
    "metrics = evaluate_classification(image_features, text_features, labels)\n",
    "print(metrics)\n",
    "\n",
    "reduce_and_visualize(image_features.numpy(), text_features.numpy(), methods=['svd', 'pca', 'tsne', 'umap'], n_dim=2)\n",
    "\n",
    "all_metrics = move_features(image_features, text_features, partial(evaluate_classification, labels=labels), direction_vec)\n",
    "plot_metrics(all_metrics, metric_name='Hit@1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval\n",
    "\n",
    "model = load_clip()\n",
    "dataset = ImageCaptionDataset()\n",
    "image_features, text_features = encode_clip(model, dataset)\n",
    "metrics = evaluate_retrieval(image_features, text_features)\n",
    "print(metrics)\n",
    "reduce_and_visualize(image_features.numpy(), text_features.numpy(), methods=['svd', 'pca', 'tsne', 'umap'], n_dim=2)\n",
    "\n",
    "all_metrics = move_features(image_features, text_features, evaluate_retrieval)\n",
    "plot_metrics(all_metrics, metric_name='Image R@1')\n",
    "plot_metrics(all_metrics, metric_name='Text R@1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageCaptionDataset(split='train', max_data_size=50000)\n",
    "model = load_clip()\n",
    "model.logit_scale.data = torch.log(torch.tensor(100))\n",
    "logs, model = train_clip_toy(model, dataset, f'ANONYMOUS_ROOTDIR/develop/open-world/exps/pretrained_512d_refactor_t100/', batch_size=64, end_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageCaptionDataset()\n",
    "model = load_clip()\n",
    "logs, model = train_clip_toy(model, dataset, f'ANONYMOUS_ROOTDIR/develop/open-world/exps/pretrained_512d_uniform_refactor/', loss_funcs=[ce_loss, uniform_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageCaptionDataset()\n",
    "model = load_clip()\n",
    "logs, model = train_clip_toy(model, dataset, f'ANONYMOUS_ROOTDIR/develop/open-world/exps/pretrained_512d_dualloss_refactor/', loss_funcs=[dual_ce_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageCaptionDataset()\n",
    "model = load_clip()\n",
    "logs, model = train_clip_toy(model, dataset, f'ANONYMOUS_ROOTDIR/develop/open-world/exps/pretrained_512d_removehard_refactor/', loss_funcs=[simple_ce_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream Task using Fine-tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_clip('ANONYMOUS_ROOTDIR/develop/open-world/exps/pretrained_512d_refactor_t30/model_epoch_1.pt')\n",
    "# dataset = ImageCaptionDataset(split='train', max_data_size=50000)\n",
    "# dataset.data = dataset.data[:500]\n",
    "dataset = ImageCaptionDataset(split='val')\n",
    "image_features, text_features = encode_clip(model, dataset)\n",
    "feature_dist = (image_features.mean(axis=0) - text_features.mean(axis=0)).norm().item()\n",
    "print(feature_dist)\n",
    "metrics = evaluate_retrieval(image_features, text_features)\n",
    "print(metrics)\n",
    "reduce_and_visualize(image_features.numpy(), text_features.numpy(), methods=['svd', 'pca'], n_dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_clip('ANONYMOUS_ROOTDIR/develop/open-world/exps/pretrained_512d_refactor_t30/model_epoch_1.pt')\n",
    "dataset = ClassificationDataset(name='CIFAR10')\n",
    "image_features, text_features = encode_clip_classification(model, dataset)\n",
    "feature_dist = (image_features.mean(axis=0) - text_features.mean(axis=0)).norm().item()\n",
    "print(feature_dist)\n",
    "labels = [item[1] for item in dataset]\n",
    "metrics = evaluate_classification(image_features, text_features, labels)\n",
    "print(metrics)\n",
    "reduce_and_visualize(image_features.numpy(), text_features.numpy(), methods=['svd', 'pca'], n_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "gaps = [0.2384, 0.3028, 0.5524, 0.6352, 0.7961, 1.0006]\n",
    "acc_i = [0.0214, 0.1896, 0.1772, 0.2048, 0.2090, 0.1836]\n",
    "acc_t = [0.0170, 0.1660, 0.1740, 0.2098, 0.2036, 0.1894]\n",
    "xs = [1, 1/10, 1/20, 1/30, 1/50, 1/100]\n",
    "plt.plot(xs, gaps, 'o-', label='Gap')\n",
    "plt.plot(xs, acc_i, 'o-', label='Image R@1')\n",
    "plt.plot(xs, acc_t, 'o-', label='Text R@1')\n",
    "# plt a line at y=0.8262\n",
    "plt.axhline(y=0.8262, color='k', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('Temperature')\n",
    "\n",
    "plt.figure()\n",
    "gaps = [0.9407, 0.6450, 0.8455, 0.9346, 1.0092, 1.1241]\n",
    "acc = [0.1918, 0.5036, 0.4525, 0.4544, 0.5065, 0.3348]\n",
    "plt.plot(xs, gaps, 'o-', label='Gap')\n",
    "plt.plot(xs, acc, 'o-', label='Acc')\n",
    "plt.axhline(y=1.1136, color='k', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('Temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap vs Prediction Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((preds1 == preds2).float().mean())\n",
    "sim1 = image_features1 @ text_features1.t()\n",
    "sim2 = image_features2 @ text_features2.t()\n",
    "\n",
    "overlaps = []\n",
    "for idx in range(len(sim1)):\n",
    "    top_preds1 = sim1[idx].argsort().tolist()[::-1][:5]\n",
    "    for pred in top_preds1: print(dataset.data[pred])\n",
    "    print()\n",
    "    top_preds2 = sim2[idx].argsort().tolist()[::-1][:5]\n",
    "    for pred in top_preds2: print(dataset.data[pred])\n",
    "    overlap = len(set(top_preds1) & set(top_preds2)) / len(set(top_preds1) | set(top_preds2))\n",
    "    overlaps.append(overlap)\n",
    "    break\n",
    "\n",
    "# print(np.mean(overlaps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_clip('ANONYMOUS_ROOTDIR/develop/open-world/exps/random_t100/model_epoch_1.pt')\n",
    "dataset = ImageCaptionDataset(split='train', max_data_size=50000)\n",
    "dataset.data = dataset.data[:500]\n",
    "# dataset = ImageCaptionDataset(split='val')\n",
    "image_features, text_features = encode_clip(model, dataset)\n",
    "feature_dist = (image_features.mean(axis=0) - text_features.mean(axis=0)).norm().item()\n",
    "print(feature_dist)\n",
    "metrics = evaluate_retrieval(image_features, text_features)\n",
    "print(metrics)\n",
    "reduce_and_visualize(image_features.numpy(), text_features.numpy(), methods=['svd', 'pca'], n_dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_clip('ANONYMOUS_ROOTDIR/develop/open-world/exps/random_t100_fix_init/model_epoch_1.pt')\n",
    "dataset = ImageCaptionDataset(split='train', max_data_size=50000)\n",
    "dataset.data = dataset.data[:500]\n",
    "w, _, _ = torch.load('ANONYMOUS_ROOTDIR/develop/open-world/exps/random_t100_fix_init/w.pt')\n",
    "# dataset = ImageCaptionDataset(split='val')\n",
    "image_features, text_features = encode_clip(model, dataset)\n",
    "text_features = text_features @ w.T\n",
    "feature_dist = (image_features.mean(axis=0) - text_features.mean(axis=0)).norm().item()\n",
    "print(feature_dist)\n",
    "metrics = evaluate_retrieval(image_features, text_features)\n",
    "print(metrics)\n",
    "reduce_and_visualize(image_features.numpy(), text_features.numpy(), methods=['svd', 'pca'], n_dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
